# -*- coding: utf-8 -*-
"""Similarité_transition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfNi3RemiLfJ6hoFJP2J-O3e4hlxaCFe
"""

job_transition = pd.read_csv('Normalized Job transition.csv') ########### remplacer par votre fichier contenant vos transitions de jobs

class EscoDataset(Dataset):
    def __init__(self, df, skill_col, backbone):
        texts = df
        self.tokenizer = AutoTokenizer.from_pretrained(backbone)
        self.texts = texts[skill_col].values.tolist()

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        res = self.tokenizer(
            self.texts[idx],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=20
        )
        return {k:v[0] for k,v in res.items()}

class ClsPool(nn.Module):
    def forward(self, x):
        return x[:, 0, :]


class BertModel(nn.Module):
    def __init__(self, backbone):
        super().__init__()

        self.backbone_name = backbone
        self.backbone = AutoModel.from_pretrained(backbone)
        self.pool = ClsPool()

    def forward(self, x):
        x = self.backbone(**x)["last_hidden_state"]
        x = self.pool(x)

        return x
backbone = 'jjzha/jobbert-base-cased'
emb_label = 'job1bert'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ds = EscoDataset(job_transition, 'job1 esco', backbone)
dl = DataLoader(ds, shuffle=False, batch_size=32)
model = BertModel(backbone)
model.eval()
model.to(device)

embs = []
with torch.no_grad():
    for i, x in enumerate(dl):
        x = {k:v.to(device) for k, v in x.items()}
        out = model(x)
        embs.extend(out.detach().cpu())
job_transition[emb_label] = embs

backbone = 'jjzha/jobbert-base-cased'
emb_label = 'job2bert'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

ds = EscoDataset(job_transition, 'job2 esco', backbone)
dl = DataLoader(ds, shuffle=False, batch_size=32)

model = BertModel(backbone)
model.eval()
model.to(device)
embs = []
with torch.no_grad():
    for i, x in enumerate(dl):
        x = {k:v.to(device) for k, v in x.items()}
        out = model(x)
        embs.extend(out.detach().cpu())
job_transition[emb_label] = embs

def compute_similarity_opt(job1_emb, job2_emb):

    norm_job1_emb = torch.nn.functional.normalize(job1_emb, p=2, dim=0)
    norm_job2_emb = torch.nn.functional.normalize(job2_emb, p=2, dim=0)

    sim = cosine_similarity(norm_job1_emb.reshape(1, -1), norm_job2_emb.reshape(1, -1))
    return sim.item()

# Calculer la similarité pour chaque ligne
job_transition['similarity'] = job_transition.apply(lambda row: compute_similarity_opt(row['job1bert'], row['job2bert']), axis=1)
job_transition= job_transition.drop(columns=["job1bert","job2bert"])