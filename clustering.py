# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YALcrpZUKWZzDMg3KcEVgZalRpmRvyUY
"""

import pandas as pd
import re
import spacy
import numpy as np
!pip install sentence_transformers
!pip install hdbscan sentence-transformers
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer
import hdbscan
from sklearn.manifold import trustworthiness
!pip install umap-learn
import umap
import matplotlib.pyplot as plt

Explorer_df= pd.read_csv("Indeed 2019.csv")
Explorer_df['Job Description'] = Explorer_df['Job Description'].apply(lambda x: '\n'.join(x.split('\n')[1:]))
Explorer_df['Job Description'] = Explorer_df['Job Description'].apply(lambda x: re.sub(r'<br>|</div>|<p>|</p>|<b>|\n|<div>|</b>|</h2>|</h1>|<li>|</li>|</ul>', '', x))
colonnes_a_garder = ['Job Title', 'Job Description', 'Companydescription']
Explorer_df = Explorer_df.loc[:, colonnes_a_garder]
Explorer_df= Explorer_df.drop_duplicates(subset='Job Description')


nlp = spacy.load('en_core_web_sm')

def preprocess(text):
    text = text.lower()
    text = re.sub(r'\W+', ' ', text)
    doc = nlp(text)

    words_to_remove = {'job', 'description', 'summary', 'title'}
    tokens = [
        token.text for token in doc
        if not token.is_stop
        and token.ent_type_ not in {'GPE', 'ORG'}
        and token.text not in words_to_remove
    ]
    cleaned_text = ' '.join(tokens)
    return cleaned_text


Explorer_df['cleaned_description'] = Explorer_df['Job Description'].apply(preprocess)


def words_after_automate(text):
    doc = nlp(text)
    found_automate = False
    words_after_automate = []

    for token in doc:
        if found_automate:
            if len(words_after_automate) < 10:
                words_after_automate.append(token.text)
            else:
                break
        if token.lemma_ == 'automate':
            found_automate = True

    return ' '.join(words_after_automate)

Explorer_df['words_after_automate'] = Explorer_df['cleaned_description'].apply(words_after_automate)
Explorer_df.replace('', pd.NA, inplace=True)
Explorer_df.dropna(subset=['words_after_automate'], inplace=True)
automation_df = Explorer_df[['words_after_automate']].copy()

# Charger le modèle Sentence-BERT
model = SentenceTransformer('all-MiniLM-L6-v2')

# Encoder les descriptions de tâches
task_embeddings = model.encode(automation_df['words_after_automate'].tolist())

# Réduction de dimension avec UMAP
umap_model = umap.UMAP(n_components=25, random_state=42)
task_embeddings_umap = umap_model.fit_transform(task_embeddings)

# Appliquer HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=20, prediction_data=True)
automation_df['clusterHDBSCAN'] = clusterer.fit_predict(task_embeddings_umap)

# Afficher les groupes de tâches similaires
grouped_tasks = automation_df.groupby('clusterHDBSCAN')['words_after_automate'].apply(lambda tasks: ' '.join(tasks)).reset_index()



# Calcul de la trustworthiness pour différents nombres de composants PCA
n_components_list = [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 50]
trustworthiness_scores = {}

for n in n_components_list:
    umap_model = umap.UMAP(n_components=n, random_state=42)
    task_embeddings_umap = umap_model.fit_transform(task_embeddings)
    trust = trustworthiness(task_embeddings, task_embeddings_umap, n_neighbors=5)
    trustworthiness_scores[n] = trust

# Affichage des scores de trustworthiness
for n, score in trustworthiness_scores.items():
    print(f"Trustworthiness for {n} components: {score}")

# Créer un graphique de dispersion des clusters
plt.figure(figsize=(10, 8))
scatter = plt.scatter(task_embeddings_umap[:, 0], task_embeddings_umap[:, 1], c=automation_df['clusterHDBSCAN'], cmap='Spectral', s=5)
plt.colorbar(scatter, label='Cluster Label')
plt.xlabel('UMAP Component 1')
plt.ylabel('UMAP Component 2')
plt.title('HDBSCAN Clusters Visualization')
plt.show()

# Print 10 exemples pour chaque cluster
for cluster_label in sorted(automation_df['clusterHDBSCAN'].unique()):
    print(f"Cluster {cluster_label}:")
    cluster_tasks = automation_df[automation_df['clusterHDBSCAN'] == cluster_label]['words_after_automate'].head(10).tolist()
    for task in cluster_tasks:
        print(f"- {task}")
    print("\n")