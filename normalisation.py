# -*- coding: utf-8 -*-
"""Normalisation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jJmj_16ij8T8_XTX8pZJfKl3UZlgPoAy
"""

import os
import re
import time

import h5py
import matplotlib.pyplot as plt
import nltk
from nltk.tokenize import sent_tokenize
from nltk import ngrams
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
from bs4 import BeautifulSoup
import torch
from torch.utils.data import Dataset, DataLoader
from torch import nn
import transformers
from transformers import pipeline, AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity


nltk.download('punkt')
esco_df= pd.read_csv("ESCO.csv")
job_esco = pd.DataFrame({'Job esco': esco_df['Job esco'].drop_duplicates()})
job_esco= job_esco.reset_index(drop=True)

class EscoDataset(Dataset):
    def __init__(self, df, skill_col, backbone):
        texts = df
        self.tokenizer = AutoTokenizer.from_pretrained(backbone)
        self.texts = texts[skill_col].values.tolist()

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        res = self.tokenizer(
            self.texts[idx],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=20
        )
        return {k:v[0] for k,v in res.items()}

class ClsPool(nn.Module):
    def forward(self, x):

        return x[:, 0, :]


class BertModel(nn.Module):
    def __init__(self, backbone):
        super().__init__()

        self.backbone_name = backbone
        self.backbone = AutoModel.from_pretrained(backbone)
        self.pool = ClsPool()

    def forward(self, x):
        x = self.backbone(**x)["last_hidden_state"]
        x = self.pool(x)

        return x

backbone = 'jjzha/jobbert-base-cased'
emb_label = 'jobbert'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


ds = EscoDataset(job_esco, 'Job esco', backbone)
dl = DataLoader(ds, shuffle=False, batch_size=32)


model = BertModel(backbone)
model.eval()
model.to(device)


embs = []
with torch.no_grad():
    for i, x in enumerate(dl):
        x = {k:v.to(device) for k, v in x.items()}
        out = model(x)
        embs.extend(out.detach().cpu())

job_esco[emb_label] = embs

linkedin= pd.read_csv("ALL_Title_Skills.csv") ##### remplacer par son fichier contenant les jobs que l'on veut normaliser
linkedin = pd.DataFrame({'Job': linkedin['Job'].drop_duplicates()})
linkedin=linkedin.dropna()
linkedin= linkedin.reset_index(drop=True)
backbone = 'jjzha/jobbert-base-cased'
emb_label = 'jobbert'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

ds = EscoDataset(linkedin, 'Job', backbone)
dl = DataLoader(ds, shuffle=False, batch_size=32)

model = BertModel(backbone)
model.eval()
model.to(device)

embs = []
with torch.no_grad():
    for i, x in enumerate(dl):
        x = {k:v.to(device) for k, v in x.items()}
        out = model(x)
        embs.extend(out.detach().cpu())

linkedin[emb_label] = embs


def calculate_similarity_matrix(df1, df2):
    similarity_matrix = np.zeros((len(df1), len(df2)))
    for i, emb1 in enumerate(df1['jobbert']):
        for j, emb2 in enumerate(df2['jobbert']):
            similarity_matrix[i, j] = cosine_similarity([emb1], [emb2])[0][0]
    return similarity_matrix

cosine_similarity_matrix = calculate_similarity_matrix(linkedin, job_esco)

most_similar_indices = np.argmax(cosine_similarity_matrix, axis=1)
similarities = np.max(cosine_similarity_matrix, axis=1)
linkedin['job esco similaire'] = job_esco.iloc[most_similar_indices]['Job esco'].values
linkedin['similarity'] = similarities

Linkedin=linkedin[["Job",'job esco similaire','similarity']]
linkedin['job esco similaire'] = job_esco.iloc[most_similar_indices]['Job esco'].values
linkedin['similarity'] = similarities

Linkedin=linkedin[["Job",'job esco similaire','similarity']]